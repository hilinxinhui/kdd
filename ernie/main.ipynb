{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tqdm\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 清洗无效字符\n",
    "def clean_text(text):\n",
    "    # text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    # text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(' ', '')\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# 定义读取数据集函数\n",
    "def read_data(filepath, is_one_hot=True):\n",
    "    f = open(filepath)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split(',', 1)\n",
    "        # 多分类标签One-hot处理\n",
    "        if is_one_hot:\n",
    "            label_true = data[1].strip('\"').strip('[').strip(']').replace(\"'\", '').replace(' ', '').split(',')\n",
    "            # labels = [float(1) if str(i) in data[1].split(',') else float(0) for i in range(8)]\n",
    "            labels = [float(1) if label_vocab[i] in label_true else float(0) for i in range(len(label_vocab))]\n",
    "        else:\n",
    "            labels = [int(d) for d in data[1].split(',')]\n",
    "        yield {\"text\": clean_text(data[0]), \"labels\": labels}\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "dataset_path = './work/data/Train.csv'\n",
    "dataset = pd.read_csv(dataset_path, encoding='gb18030')\n",
    "dataset = dataset.drop('ID', axis=1)\n",
    "# print(dataset.info())\n",
    "# print(dataset.head(5))\n",
    "\n",
    "# # 创建训练集和测试集\n",
    "train_ds, test_ds = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "train_ds.to_csv('./work/data/train.csv', header=False, index=False)\n",
    "test_ds.to_csv('./work/data/test.csv', header=False, index=False)\n",
    "\n",
    "# 标签字典\n",
    "# labels_list = dataset['Labels'].apply(ast.literal_eval).tolist()\n",
    "# labels = list(set([item for sublist in labels_list for item in sublist]))\n",
    "# label_vocab = {}\n",
    "# for i in range(len(labels)):\n",
    "#     label_vocab[i] = labels[i]\n",
    "# print(label_vocab) # {0: 'Anger', 1: 'Expect', 2: 'Love', 3: 'Sorrow', 4: 'Anxiety', 5: 'Hate', 6: 'Joy', 7: 'Surprise'}\n",
    "label_vocab = {0: 'Anger', 1: 'Expect', 2: 'Love', 3: 'Sorrow', 4: 'Anxiety', 5: 'Hate', 6: 'Joy', 7: 'Surprise'}\n",
    "\n",
    "# # 读取训练集和测试集\n",
    "train_ds = load_dataset(read_data, filepath='./work/data/train.csv', lazy=False)\n",
    "test_ds = load_dataset(read_data, filepath='./work/data/test.csv', lazy=False)\n",
    "# print(type(train_ds), type(test_ds))\n",
    "print(train_ds[3])\n",
    "print(test_ds[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-xbase-zh\"\n",
    "num_classes = len(label_vocab)  # 8分类任务\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=100)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=16, shuffle=True)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=16, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from paddle.metric import Metric\n",
    "\n",
    "# 自定义MultiLabelReport评价指标\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='MultiLabelReport', average='micro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def f1_score(self, y_prob):\n",
    "        '''\n",
    "        Returns the f1 score by searching the best threshhold\n",
    "        '''\n",
    "        best_score = 0\n",
    "        for threshold in [i * 0.01 for i in range(100)]:\n",
    "            self.y_pred = y_prob > threshold\n",
    "            score = sklearn.metrics.f1_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                precison = precision_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "                recall = recall_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        return best_score, precison, recall\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        auc = roc_auc_score(\n",
    "            y_score=self.y_prob, y_true=self.y_true, average=self.average)\n",
    "        f1_score, precison, recall = self.f1_score(y_prob=self.y_prob)\n",
    "        return auc, f1_score, precison, recall\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# AdamW优化器、交叉熵损失函数、自定义MultiLabelReport评价指标\n",
    "lr = 2e-5\n",
    "lr_scheduler = paddle.optimizer.lr.StepDecay(learning_rate=lr, step_size=1, gamma=0.5)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=lr_scheduler, parameters=model.parameters(), weight_decay=0.01)\n",
    "criterion = paddle.nn.BCEWithLogitsLoss()\n",
    "metric = MultiLabelReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 构建验证集evaluate函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        losses.append(loss.numpy())\n",
    "        metric.update(probs, labels)\n",
    "        if if_return_results:\n",
    "            probs = probs.tolist()\n",
    "            for prob in probs:\n",
    "                result = []\n",
    "                for c, pred in enumerate(prob):\n",
    "                    if pred > 0.5:\n",
    "                        result.append(label_vocab[c])\n",
    "                results.append(','.join(result))\n",
    "\n",
    "    auc, f1_score, precison, recall = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, auc: %.5f, f1 score: %.5f, precison: %.5f, recall: %.5f\" %\n",
    "          (np.mean(losses), auc, f1_score, precison, recall))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    if if_return_results:\n",
    "        return results\n",
    "    else:\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 4 # 训练轮次\n",
    "ckpt_dir = \"ckpt\" # 训练过程中保存模型参数的文件夹\n",
    "\n",
    "global_step = 0  # 迭代次数\n",
    "tic_train = time.time()\n",
    "best_f1_score = 0\n",
    "\n",
    "# 模型训练\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率、f1分数\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        metric.update(probs, labels)\n",
    "        auc, f1_score, _, _ = metric.accumulate()\n",
    "\n",
    "        # 每迭代10次，打印损失函数值、准确率、f1分数、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 50 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, auc, f1_score,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # 每迭代40次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
    "        if global_step % 100 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            eval_f1_score = evaluate(model, criterion, metric, test_data_loader, label_vocab, if_return_results=False)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型验证\n",
    "results = evaluate(model, criterion, metric, test_data_loader, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据加载和处理函数\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "def convert_example(example, tokenizer, max_seq_length=64, is_test=False):\n",
    "    qtconcat = example[\"text\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, label_vocab, batch_size=1, max_seq=64):\n",
    "    examples = []\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=max_seq,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        # print(len(logits))\n",
    "        probs = F.sigmoid(logits)\n",
    "        probs = probs.tolist()\n",
    "        # 结果处理,选取概率大于0.5的情感类别\n",
    "        for idx, prob in enumerate(probs):\n",
    "            result = []\n",
    "            for c, pred in enumerate(prob):\n",
    "                if pred > 0.45:\n",
    "                    result.append(label_vocab[c])\n",
    "            results.append(','.join(result))\n",
    "    return results  # 返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./work/test.csv\", encoding='gb18030')\n",
    "data = data.drop('ID', axis=1)\n",
    "data.to_csv(\"./work/data.csv\", header=False, index=False)\n",
    "\n",
    "data = data.values\n",
    "for_test = []\n",
    "for i in data:\n",
    "    for_test.append({\"text\": i[0]})\n",
    "# print(for_test)\n",
    "\n",
    "# 模型预测\n",
    "labels =  predict(model, for_test, tokenizer, label_vocab, batch_size=1)\n",
    "\n",
    "\n",
    "# 保存预测结果\n",
    "submit = pd.DataFrame(columns=['ID', 'Labels'])\n",
    "for idx, label in enumerate(labels):\n",
    "    # print(idx + 1, type(label))\n",
    "    label = label.split(\",\")\n",
    "    if len(label) == 1:\n",
    "        s = '\"[' + label[0] + ']\"'\n",
    "        submit = submit.append(pd.DataFrame([[idx + 1, s]], columns=submit.columns))\n",
    "        continue    \n",
    "    submit = submit.append(pd.DataFrame([[idx + 1, label]], columns=submit.columns))\n",
    "\n",
    "submit.to_csv(\"./submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
