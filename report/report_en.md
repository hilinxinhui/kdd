## Abstract

Sentiment analysis plays a crucial role in natural language processing tasks. In recent years, sentiment analysis tasks on text have evolved from simple multi-class problems, such as positive-negative-neutral classification, to more fine-grained multi-label problems, demanding classifiers to output specific emotions contained in the text. This shift presents challenges for language models. The prevalent approach in the field of natural language processing involves the use of pre-training and fine-tuning strategies. This strategy often entails constructing a model with numerous parameters, conducting self-supervised auto-regression training on a large general corpus, and subsequently fine-tuning it on specific datasets for particular tasks.The emergence of language models based on pre-training and fine-tuning strategies is becoming increasingly prominent. This approach involves constructing a model with numerous parameters and conducting self-supervised regression-style training on a vast general corpus. The model is then fine-tuned on specific datasets for particular tasks, and the resulting fine-tuned model is employed for inference.This paper, utilizing the Ernie 3.0 base model, implements multi-label sentiment analysis on a given dataset. The model achieves a macro F1-Score of 0.67598 on the validation set and a macro F1 score of 0.72736 on the test set. In the competition, the model ranks 18th (without deduplication) and 9th (with deduplication). Experimental and competition results demonstrate the feasibility of this pre-training-fine-tuning paradigm in sentiment analysis tasks, showcasing high accuracy in inference.

keywords: sentiment analysis, multi-label classification, pre-trained model